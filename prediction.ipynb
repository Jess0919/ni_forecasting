{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score as AUC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "import features"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## features engineering"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# generate features\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "ni_raw = pd.read_feather('ni.feather')\n",
    "ni_i = features.first_and_last(ni_raw)\n",
    "features.make_minute(ni_i)\n",
    "features.make_datetime(ni_i)\n",
    "features.make_labels(ni_i)\n",
    "\n",
    "ni = pd.read_feather('ni_minute.feather')\n",
    "features.make_diff_period_close_features(ni['Close', 'price_max', 'price_min'])\n",
    "features.make_prices_features(ni['price_mean', 'ask_mean', 'bid_mean'])\n",
    "features.make_qty_features(ni[['volume_mean', 'ask_qty_mean', 'bid_qty_mean', 'open_int_mean']])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#  predict next 1 minute\n",
    "ni_features_labels = pd.DataFrame()\n",
    "\n",
    "ni_minute = pd.read_feather('date.feather')\n",
    "ni_features_labels['date'] = ni_minute['date']\n",
    "ni_features_labels['datetime'] = ni_minute['datetime']\n",
    "\n",
    "# close features\n",
    "for f in ['close']:\n",
    "    features = pd.read_feather(f + '_features.feather')\n",
    "    for slot in [1, 2, 5]:\n",
    "        for col in ['RSI3', 'RSI6', 'RSI12', 'RSI24', 'MOM', 'DIFF', 'MACD', 'talib_K', 'talib_D', 'talib_J']:\n",
    "            ni_features_labels[f + '_' + col + '_m' + str(slot)] = features[col + '_m' + str(slot)]\n",
    "\n",
    "# price, ask, bid features\n",
    "for f in ['price', 'ask', 'bid']:\n",
    "    features = pd.read_feather(f + '_features.feather')\n",
    "    for slot in []:\n",
    "        for m in range(20):\n",
    "            ni_features_labels[f + '_mean' + str(m) + '_m' + str(slot)] = features[f + '_mean' + str(m) + '_m' + str(slot)]\n",
    "    for slot in [1]:\n",
    "        for m in range(20):\n",
    "            ni_features_labels[f + '_rate' + str(m) + '_m' + str(slot)] = features[f + '_rate' + str(m) + '_m' + str(slot)]\n",
    "\n",
    "# volume, ast_qty, bid_qty features\n",
    "for f in ['volume', 'ask_qty', 'bid_qty']:\n",
    "    features = pd.read_feather(f + '_features.feather')\n",
    "    for slot in [1]:\n",
    "        for m in range(20):\n",
    "            ni_features_labels[f + '_mean' + str(m) + '_m' + str(slot)] = features[f + '_mean' + str(m) + '_m' + str(slot)]\n",
    "    for slot in [1, 2, 5, 10]:\n",
    "        for m in range(20):\n",
    "            ni_features_labels[f + '_rate' + str(m) + '_m' + str(slot)] = features[f + '_rate' + str(m) + '_m' + str(slot)]\n",
    "\n",
    "# open_int features\n",
    "features = pd.read_feather('open_int_features.feather')\n",
    "for slot in []:\n",
    "    for m in range(20):\n",
    "        ni_features_labels['open_int_mean' + str(m) + '_m' + str(slot)] = features['open_int_mean' + str(m) + '_m' + str(slot)]\n",
    "for slot in [1, 2, 5, 10]:\n",
    "    for m in range(20):\n",
    "        ni_features_labels['open_int_rate' + str(m) + '_m' + str(slot)] = features['open_int_rate' + str(m) + '_m' + str(slot)]\n",
    "\n",
    "ni_labels = pd.read_feather('ni_labels.feather')\n",
    "ni_features_labels = pd.concat([ni_features_labels, ni_labels], axis = 1)\n",
    "\n",
    "ni_features_labels = ni_features_labels.dropna(axis=0, how='any')\n",
    "ni_features_labels.reset_index(inplace=True, drop=True)\n",
    "ni_features_labels.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# predict next 5 minutes\n",
    "ni_features_labels = pd.DataFrame()\n",
    "\n",
    "ni_minute = pd.read_csv('ni_minute_1.csv')\n",
    "ni_features_labels['date'] = ni_minute['date']\n",
    "\n",
    "# close features\n",
    "for f in ['close']:\n",
    "    features = pd.read_csv(f + '_features.csv')\n",
    "    for slot in [1, 5]:\n",
    "        for col in ['RSI3', 'RSI6', 'RSI12', 'RSI24', 'MOM', 'DIFF', 'MACD', 'talib_K', 'talib_D', 'talib_J']:\n",
    "            ni_features_labels[f + '_' + col + '_m' + str(slot)] = features[col + '_m' + str(slot)]\n",
    "\n",
    "# price, ask, bid features\n",
    "for f in ['price', 'ask', 'bid']:\n",
    "    features = pd.read_csv(f + '_features.csv')\n",
    "    for slot in []:\n",
    "        for m in range(20):\n",
    "            ni_features_labels[f + '_mean' + str(m) + '_m' + str(slot)] = features[f + '_mean' + str(m) + '_m' + str(slot)]\n",
    "    for slot in [1, 5]:\n",
    "        for m in range(20):\n",
    "            ni_features_labels[f + '_rate' + str(m) + '_m' + str(slot)] = features[f + '_rate' + str(m) + '_m' + str(slot)]\n",
    "\n",
    "# volume, ast_qty, bid_qty features\n",
    "for f in ['volume', 'ask_qty', 'bid_qty']:\n",
    "    features = pd.read_csv(f + '_features.csv')\n",
    "    for slot in [1, 5]:\n",
    "        for m in range(20):\n",
    "            ni_features_labels[f + '_mean' + str(m) + '_m' + str(slot)] = features[f + '_mean' + str(m) + '_m' + str(slot)]\n",
    "    for slot in [1, 5, 10, 30, 60, 120]:\n",
    "        for m in range(20):\n",
    "            ni_features_labels[f + '_rate' + str(m) + '_m' + str(slot)] = features[f + '_rate' + str(m) + '_m' + str(slot)]\n",
    "\n",
    "# open_int features\n",
    "features = pd.read_csv('open_int_features.csv')\n",
    "for slot in [1, 5]:\n",
    "    for m in range(20):\n",
    "        ni_features_labels['open_int_mean' + str(m) + '_m' + str(slot)] = features['open_int_mean' + str(m) + '_m' + str(slot)]\n",
    "for slot in [1, 5, 10, 30, 60, 120]:#10\n",
    "    for m in range(20):\n",
    "        ni_features_labels['open_int_rate' + str(m) + '_m' + str(slot)] = features['open_int_rate' + str(m) + '_m' + str(slot)]\n",
    "\n",
    "ni_labels = pd.read_csv('ni_label.csv')\n",
    "ni_features_labels = pd.concat([ni_features_labels, ni_labels], axis = 1)\n",
    "\n",
    "# ni_features_labels.insert(0, 'index', range(len(ni_features_labels)), allow_duplicates=False)\n",
    "# ni_features_labels = ni_features_labels.loc[ni_features_labels['index'] % 5 == 0]\n",
    "# ni_features_labels.drop(columns=['index'], inplace=True)\n",
    "ni_features_labels = ni_features_labels.dropna(axis=0, how='any')\n",
    "ni_features_labels.reset_index(inplace=True, drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# predict next 10 minutes\n",
    "ni_features_labels = pd.DataFrame()\n",
    "\n",
    "ni_minute = pd.read_csv('ni_minute_1.csv')\n",
    "ni_features_labels['date'] = ni_minute['date']\n",
    "\n",
    "# close features\n",
    "for f in ['close']:\n",
    "    features = pd.read_csv(f + '_features.csv')\n",
    "    for slot in [1, 5, 10]:\n",
    "        for col in ['RSI3', 'RSI6', 'RSI12', 'RSI24', 'MOM', 'DIFF', 'MACD', 'talib_K', 'talib_D', 'talib_J']:\n",
    "            ni_features_labels[f + '_' + col + '_m' + str(slot)] = features[col + '_m' + str(slot)]\n",
    "\n",
    "# price, ask, bid features\n",
    "for f in ['price', 'ask', 'bid']:\n",
    "    features = pd.read_csv(f + '_features.csv')\n",
    "    for slot in []:\n",
    "        for m in range(20):\n",
    "            ni_features_labels[f + '_mean' + str(m) + '_m' + str(slot)] = features[f + '_mean' + str(m) + '_m' + str(slot)]\n",
    "    for slot in [1]:\n",
    "        for m in range(20):\n",
    "            ni_features_labels[f + '_rate' + str(m) + '_m' + str(slot)] = features[f + '_rate' + str(m) + '_m' + str(slot)]\n",
    "\n",
    "# volume, ast_qty, bid_qty features\n",
    "for f in ['volume', 'ask_qty', 'bid_qty']:\n",
    "    features = pd.read_csv(f + '_features.csv')\n",
    "    for slot in [1, 5]:\n",
    "        for m in range(20):\n",
    "            ni_features_labels[f + '_mean' + str(m) + '_m' + str(slot)] = features[f + '_mean' + str(m) + '_m' + str(slot)]\n",
    "    for slot in [1, 5, 10, 30, 60, 120, 240]:\n",
    "        for m in range(20):\n",
    "            ni_features_labels[f + '_rate' + str(m) + '_m' + str(slot)] = features[f + '_rate' + str(m) + '_m' + str(slot)]\n",
    "\n",
    "# open_int features\n",
    "features = pd.read_csv('open_int_features.csv')\n",
    "for slot in []:\n",
    "    for m in range(20):\n",
    "        ni_features_labels['open_int_mean' + str(m) + '_m' + str(slot)] = features['open_int_mean' + str(m) + '_m' + str(slot)]\n",
    "for slot in [1, 5, 10, 30, 60, 120]:#10\n",
    "    for m in range(20):\n",
    "        ni_features_labels['open_int_rate' + str(m) + '_m' + str(slot)] = features['open_int_rate' + str(m) + '_m' + str(slot)]\n",
    "\n",
    "ni_labels = pd.read_csv('ni_label.csv')\n",
    "ni_features_labels = pd.concat([ni_features_labels, ni_labels], axis = 1)\n",
    "\n",
    "# ni_features_labels.insert(0, 'index', range(len(ni_features_labels)), allow_duplicates=False)\n",
    "# ni_features_labels = ni_features_labels.loc[ni_features_labels['index'] % 5 == 0]\n",
    "# ni_features_labels.drop(columns=['index'], inplace=True)\n",
    "ni_features_labels = ni_features_labels.dropna(axis=0, how='any')\n",
    "ni_features_labels.reset_index(inplace=True, drop=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# predict next 30 minutes\n",
    "ni_features_labels = pd.DataFrame()\n",
    "\n",
    "ni_minute = pd.read_feather('date.feather')\n",
    "ni_features_labels['date'] = ni_minute['date']\n",
    "ni_features_labels['datetime'] = ni_minute['datetime']\n",
    "#ni_features_labels['datetime'] = ni_minute['datetime'].shift(-30)\n",
    "\n",
    "# close features\n",
    "for f in ['close']:\n",
    "    features = pd.read_feather(f + '_features.feather')\n",
    "    for slot in [1, 5]:\n",
    "        for col in ['RSI3', 'RSI6', 'RSI12', 'RSI24', 'MOM', 'DIFF', 'MACD', 'talib_K', 'talib_D', 'talib_J']:\n",
    "            ni_features_labels[f + '_' + col + '_m' + str(slot)] = features[col + '_m' + str(slot)]\n",
    "\n",
    "# price, ask, bid features\n",
    "for f in ['price', 'ask', 'bid']:\n",
    "    features = pd.read_feather(f + '_features.feather')\n",
    "    for slot in []:\n",
    "        for m in range(20):\n",
    "            ni_features_labels[f + '_mean' + str(m) + '_m' + str(slot)] = features[f + '_mean' + str(m) + '_m' + str(slot)]\n",
    "    for slot in [1]:\n",
    "        for m in range(20):\n",
    "            ni_features_labels[f + '_rate' + str(m) + '_m' + str(slot)] = features[f + '_rate' + str(m) + '_m' + str(slot)]\n",
    "\n",
    "# volume, ast_qty, bid_qty features\n",
    "for f in ['volume', 'ask_qty', 'bid_qty']:\n",
    "    features = pd.read_feather(f + '_features.feather')\n",
    "    for slot in [1, 10, 30]:\n",
    "        for m in range(20):\n",
    "            ni_features_labels[f + '_mean' + str(m) + '_m' + str(slot)] = features[f + '_mean' + str(m) + '_m' + str(slot)]\n",
    "    for slot in [1, 5, 10, 30, 60, 120, 240]:\n",
    "        for m in range(20):\n",
    "            ni_features_labels[f + '_rate' + str(m) + '_m' + str(slot)] = features[f + '_rate' + str(m) + '_m' + str(slot)]\n",
    "\n",
    "# open_int features\n",
    "features = pd.read_feather('open_int_features.feather')\n",
    "for slot in []:\n",
    "    for m in range(20):\n",
    "        ni_features_labels['open_int_mean' + str(m) + '_m' + str(slot)] = features['open_int_mean' + str(m) + '_m' + str(slot)]\n",
    "for slot in [1, 5, 10, 30, 60, 120, 240]:\n",
    "    for m in range(20):\n",
    "        ni_features_labels['open_int_rate' + str(m) + '_m' + str(slot)] = features['open_int_rate' + str(m) + '_m' + str(slot)]\n",
    "\n",
    "ni_labels = pd.read_feather('ni_labels.feather')\n",
    "ni_features_labels = pd.concat([ni_features_labels, ni_labels], axis = 1)\n",
    "\n",
    "ni_features_labels = ni_features_labels.dropna(axis=0, how='any')\n",
    "ni_features_labels.reset_index(inplace=True, drop=True)\n",
    "ni_features_labels.columns"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### load train data and test data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def load_data(data, label_bar, rolling_date):\n",
    "    train_start_date = rolling_date['train_start_date']\n",
    "    train_end_date = rolling_date['train_end_date']\n",
    "    test_start_date = rolling_date['test_start_date']\n",
    "    test_end_date = rolling_date['test_end_date']\n",
    "    feature_index = []\n",
    "    for col in data.columns:\n",
    "        if (not re.findall(r'label', col)) & (not re.findall(r'date', col)):\n",
    "            feature_index.append(col)\n",
    "    features = data[feature_index]\n",
    "    long_label = (data['label_long_' + str(label_bar)] >= 0).astype('int')\n",
    "    short_label = (data['label_short_' + str(label_bar)] >= 0).astype('int')\n",
    "    invest = pd.DataFrame({'long_invest': data['label_long_invest'], 'short_invest': data['label_short_invest' + str(label_bar)],\n",
    "                           'long_return': data['label_long_' + str(label_bar)], 'short_return': data['label_short_' + str(label_bar)]})\n",
    "\n",
    "    train_index = []\n",
    "    test_index = []\n",
    "    train_index.append(data.loc[data['date'] >= train_start_date & data['date'] < train_end_date].index.tolist())\n",
    "    test_index.append(data.loc[data['date'] >= test_start_date & data['date'] < test_end_date].index.tolist())\n",
    "    d = data.iloc[test_index[0]]\n",
    "    date = d[['date', 'datetime']]\n",
    "    date.reset_index(inplace=True, drop=True)\n",
    "    print(type(date))\n",
    "    date.to_feather('test_set_date.feather')\n",
    "\n",
    "    return features, long_label, short_label, train_index, test_index, invest\n",
    "\n",
    "date = {'train_start_date': datetime(2020, 1, 1),\n",
    "        'train_end_date': datetime(2020, 12, 31),\n",
    "        'test_start_date': datetime(2021, 1, 1),\n",
    "        'test_end_date': datetime(2020, 8, 31)}\n",
    "features, long_label, short_label, train_index, test_index, invest = load_data(ni_features_labels, 30, date)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(features.columns)\n",
    "num_train = (long_label.iloc[train_index[0]] == 0).sum()\n",
    "num_test = (long_label.iloc[train_index[0]] == 1).sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(features, labels):\n",
    "  train_data = lgb.Dataset(features, label=labels)\n",
    "  params = {\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'num_leaves': 100,\n",
    "        'max_depth': 12,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.6,\n",
    "        'bagging_freq': 0,\n",
    "        'seed': 100,\n",
    "        'verbose': 1,\n",
    "        'lambda_l1': 0.3,\n",
    "        'lambda_l2': 1e-03,\n",
    "        'scale_pos_weight':num_train/num_test\n",
    "  }\n",
    "  lgbm = lgb.train(params, train_data, num_boost_round=10000)\n",
    "  return lgbm\n",
    "\n",
    "def test(features, long_labels, short_labels, invest, lgbm_long, lgbm_short):\n",
    "  y_pred_long = lgbm_long.predict(features)\n",
    "  y_pred_short = lgbm_short.predict(features)\n",
    "  long_invest = np.array(invest['long_invest']).tolist()\n",
    "  short_invest = np.array(invest['short_invest']).tolist()\n",
    "  long_return = np.array(invest['long_return']).tolist()\n",
    "  short_return = np.array(invest['short_return']).tolist()\n",
    "  diff_long = pd.DataFrame({'label': long_labels, 'pred': y_pred_long,\n",
    "                            'long_invest': invest['long_invest'], 'long_return': invest['long_return']})\n",
    "  diff_long.reset_index(drop=True, inplace=True)\n",
    "  diff_long.to_feather('diff_long.feather')\n",
    "  diff_short = pd.DataFrame({'label': short_labels, 'pred': y_pred_short,\n",
    "                             'short_invest': invest['short_invest'], 'short_return': invest['short_return']})\n",
    "  diff_short.reset_index(drop=True, inplace=True)\n",
    "  diff_short.to_feather('diff_short.feather')\n",
    "  result = {}\n",
    "  result['precision_long'] = []\n",
    "  result['recall_long'] = []\n",
    "  result['return_rate_long'] = []\n",
    "  result['precision_short'] = []\n",
    "  result['recall_short'] = []\n",
    "  result['return_rate_short'] = []\n",
    "  C_long = []\n",
    "  C_short = []\n",
    "  long_label = np.array(long_labels).reshape(1, -1).tolist()[0]\n",
    "  short_label = np.array(short_labels).reshape(1, -1).tolist()[0]\n",
    "  for thr in [0.5, 0.9, 0.99, 0.995]:\n",
    "    prediction_long = (y_pred_long >= thr).astype('int')\n",
    "    prediction_short = (y_pred_short >= thr).astype('int')\n",
    "    c = confusion_matrix(long_label, prediction_long, labels=[0, 1]) # 可将'1'等替换成自己的类别，如'cat'。\n",
    "    C_long.append(c)\n",
    "    inv = 0\n",
    "    ret = 0\n",
    "    result['precision_long'].append(c[1][1]/(c[0][1]+c[1][1]))\n",
    "    result['recall_long'].append(c[1][1]/(c[1][0]+c[1][1]))\n",
    "    for i in range(len(prediction_long)):\n",
    "        if prediction_long[i] >= thr:\n",
    "            inv += long_invest[i]\n",
    "            ret += long_return[i]\n",
    "    result['return_rate_long'].append(ret/inv)\n",
    "    c = confusion_matrix(short_label, prediction_short, labels=[0, 1]) # 可将'1'等替换成自己的类别，如'cat'。\n",
    "    C_short.append(c)\n",
    "    inv = 0\n",
    "    ret = 0\n",
    "    p = 0\n",
    "    r = 0\n",
    "    p = c[1][1]/(c[0][1]+c[1][1])\n",
    "    r = c[1][1]/(c[1][0]+c[1][1])\n",
    "    result['precision_short'].append(p)\n",
    "    result['recall_short'].append(r)\n",
    "    for i in range(len(prediction_short)):\n",
    "        if prediction_short[i] >= thr:\n",
    "            inv += short_invest[i]\n",
    "            ret += short_return[i]\n",
    "    result['return_rate_short'].append(ret/inv)\n",
    "  for c in C_long:\n",
    "    print(c)\n",
    "  for c in C_short:\n",
    "    print(c)\n",
    "\n",
    "  rate_of_return = []\n",
    "  for thr in [0.9, 0.95, 0.99, 0.995]:\n",
    "      inv = 0\n",
    "      ret = 0\n",
    "      for i in range(len(prediction_long)):\n",
    "          if y_pred_long[i] - y_pred_short[i] >= thr:\n",
    "              inv += long_invest[i]\n",
    "              ret += long_return[i]\n",
    "          if y_pred_short[i] - y_pred_long[i] >= thr:\n",
    "              inv += short_invest[i]\n",
    "              ret += short_return[i]\n",
    "      rate_of_return.append(ret/inv)\n",
    "  df = pd.DataFrame(rate_of_return)\n",
    "  df.to_csv('rate_of_return.txt', index=False)\n",
    "\n",
    "  return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "invest.iloc[test_index[0]].head(20)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lgbm_long = train(features.iloc[train_index[0]], long_label.iloc[train_index[0]])\n",
    "lgbm_short = train(features.iloc[train_index[0]], short_label.iloc[train_index[0]])\n",
    "lgbm_long.save_model('lgbm_long.txt')\n",
    "lgbm_short.save_model('lgbm_short.txt')\n",
    "result = test(features.iloc[test_index[0]], long_label.iloc[test_index[0]], short_label.iloc[test_index[0]], invest.iloc[test_index[0]], lgbm_long, lgbm_short)\n",
    "\n",
    "importance = lgbm_long.feature_importance()\n",
    "names = lgbm_long.feature_name()\n",
    "with open('./features_importance.txt', 'w+') as file:\n",
    "    for index, im in enumerate(importance):\n",
    "        string = names[index] + ', ' + str(im) + '\\n'\n",
    "        file.write(string)\n",
    "\n",
    "result_df = pd.DataFrame(result)\n",
    "result_df.to_csv('result.txt', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### hyper-parameters tuning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "X_train = features.iloc[train_index[0]]\n",
    "y_train = long_label.iloc[train_index[0]]\n",
    "\n",
    "params = {\n",
    "          'boosting_type': 'gbdt',\n",
    "          'objective': 'binary',\n",
    "          'metric': 'auc',\n",
    "          'nthread':4,\n",
    "          'learning_rate':0.01,\n",
    "          'num_leaves':32,\n",
    "          'max_depth': 5,\n",
    "          'subsample': 0.8,\n",
    "          'colsample_bytree': 0.8,\n",
    "    }\n",
    "\n",
    "data_train = lgb.Dataset(X_train, y_train)\n",
    "cv_results = lgb.cv(params, data_train, num_boost_round=10000, nfold=5, stratified=False, shuffle=True, metrics='auc',early_stopping_rounds=100,seed=0)\n",
    "print('best n_estimators:', len(cv_results['auc-mean']))\n",
    "print('best cv score:', pd.Series(cv_results['auc-mean']).max())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# select max_depth and num_leaves\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "params_test1={'max_depth': range(10,18,1), 'num_leaves':range(80, 200, 20)}\n",
    "#params_test1={'num_leaves':range(80, 120, 1)}\n",
    "gsearch1 = GridSearchCV(estimator = lgb.LGBMClassifier(boosting_type='gbdt',objective='binary',metrics='auc',learning_rate=0.01, n_estimators=, max_depth=12, bagging_fraction = 0.8,feature_fraction = 0.8),\n",
    "                       param_grid = params_test1, scoring='roc_auc',cv=5,n_jobs=-1)\n",
    "gsearch1.fit(X_train,y_train)\n",
    "gsearch1.cv_results_, gsearch1.best_params_, gsearch1.best_score_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# select max_bin and min_data_in_leaf\n",
    "params_test2={'max_bin': range(5,300,10), 'min_data_in_leaf':range(1,102,10)}\n",
    "\n",
    "gsearch2 = GridSearchCV(estimator = lgb.LGBMClassifier(boosting_type='gbdt',objective='binary',metrics='auc',learning_rate=0.01, n_estimators=778, max_depth=12, num_leaves=100,bagging_fraction = 0.8,feature_fraction = 0.8),\n",
    "                       param_grid = params_test2, scoring='roc_auc',cv=5,n_jobs=-1)\n",
    "gsearch2.fit(X_train,y_train)\n",
    "gsearch2.cv_results_, gsearch2.best_params_, gsearch2.best_score_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    " params_test3={'feature_fraction': [0.6,0.7,0.8,0.9,1.0],\n",
    "              'bagging_fraction': [0.6,0.7,0.8,0.9,1.0],\n",
    "              'bagging_freq': range(0,81,10)\n",
    "}\n",
    "\n",
    "gsearch3 = GridSearchCV(estimator = lgb.LGBMClassifier(boosting_type='gbdt',objective='binary',metrics='auc',learning_rate=0.01, n_estimators=778, max_depth=12, num_leaves=100,max_bin=65,min_data_in_leaf=41),\n",
    "                       param_grid = params_test3, scoring='roc_auc',cv=5,n_jobs=-1)\n",
    "gsearch3.fit(X_train,y_train)\n",
    "gsearch3.cv_results_, gsearch3.best_params_, gsearch3.best_score_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params_test4={'lambda_l1': [1e-5,1e-3,1e-1,0.0,0.1,0.3,0.5,0.7,0.9,1.0],\n",
    "              'lambda_l2': [1e-5,1e-3,1e-1,0.0,0.1,0.3,0.5,0.7,0.9,1.0]\n",
    "}\n",
    "\n",
    "gsearch4 = GridSearchCV(estimator = lgb.LGBMClassifier(boosting_type='gbdt',objective='binary',metrics='auc',learning_rate=0.01, n_estimators=778, max_depth=12, num_leaves=100,max_bin=65,min_data_in_leaf=41,bagging_freq= 0,feature_fraction= 0.8),\n",
    "                       param_grid = params_test4, scoring='roc_auc',cv=5,n_jobs=-1)\n",
    "gsearch4.fit(X_train,y_train)\n",
    "gsearch4.cv_results_, gsearch4.best_params_, gsearch4.best_score_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "params_test5={'min_split_gain':[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]}\n",
    "\n",
    "gsearch5 = GridSearchCV(estimator = lgb.LGBMClassifier(boosting_type='gbdt',objective='binary',metrics='auc',learning_rate=0.1, n_estimators=778, max_depth=12, num_leaves=100,max_bin=65,min_data_in_leaf=41,bagging_freq= 0, feature_fraction= 0.8,\n",
    "lambda_l1=0.3,lambda_l2=1e-03),\n",
    "                       param_grid = params_test5, scoring='roc_auc',cv=5,n_jobs=-1)\n",
    "gsearch5.fit(X_train,y_train)\n",
    "gsearch5.cv_results_, gsearch5.best_params_, gsearch5.best_score_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Rolling train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gen_rolling_dates(start_date, end_date, update, train_days, test_days, slot=0):\n",
    "    train_start = start_date\n",
    "    test_start = train_start + timedelta(days=(train_days + slot))\n",
    "    rollings = []\n",
    "    while test_start < end_date:\n",
    "        rollings.append({\n",
    "            'train_start_date': train_start,\n",
    "            'train_end_date': train_start + timedelta(days=train_days),\n",
    "            'test_start_date': train_start + timedelta(days=(train_days + slot)),\n",
    "            'test_end_date': test_start + timedelta(days=test_days)\n",
    "        })\n",
    "        train_start = train_start + timedelta(days=update)\n",
    "        test_start = test_start + timedelta(days=update)\n",
    "    return rollings\n",
    "\n",
    "rollings = gen_rolling_dates(datetime.date(2020, 1, 1), datetime.date(2021, 8, 31), 60, 300, 60)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(len(rollings)):\n",
    "    features, long_label, short_label, train_index, test_index, invest = load_data(ni_features_labels, 30, rollings[i])\n",
    "    lgbm_long = train(features.iloc[train_index[0]], long_label.iloc[train_index[0]])\n",
    "    lgbm_short = train(features.iloc[train_index[0]], short_label.iloc[train_index[0]])\n",
    "    result = test(features.iloc[test_index[0]], long_label.iloc[test_index[0]], short_label.iloc[test_index[0]], invest.iloc[test_index[0]], lgbm_long, lgbm_short)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Backtesting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "date = pd.read_feather('test_set_date.feather')\n",
    "diff_long = pd.read_feather('diff_long.feather')\n",
    "diff_short = pd.read_feather('diff_short.feather')\n",
    "datetime_end = date['datetime'].shift(-30)\n",
    "df = pd.DataFrame({'date': date['date'], 'datetime_open': date['datetime'], 'datetime_close': datetime_end,\n",
    "                   'long_pred': diff_long['pred'], 'long_invest': diff_long['long_invest'], 'long_return': diff_long['long_return'],\n",
    "                   'short_pred': diff_short['pred'], 'short_invest': diff_short['short_invest'],\n",
    "                   'short_return': diff_short['short_return']})\n",
    "df['diff'] = df['long_pred'] - df['short_pred']\n",
    "df = df.loc[(df['diff'] >= 0.992) | (df['diff'] <= -0.988)]\n",
    "buy = []\n",
    "sell = 0\n",
    "open = []\n",
    "close = []\n",
    "ret = []\n",
    "for i in range(len(df)):\n",
    "    df['datetime_close'].iloc[i] = pd.to_datetime(df['datetime_close'].iloc[i]) - datetime.timedelta(seconds=1)\n",
    "    if df['diff'].iloc[i] >= 0.992:\n",
    "        buy.append(1)\n",
    "        open.append(df['long_invest'].iloc[i])\n",
    "        ret.append(df['long_return'].iloc[i])\n",
    "        close.append(df['long_invest'].iloc[i] + df['long_return'].iloc[i])\n",
    "    else:\n",
    "        buy.append(0)\n",
    "        sell += 1\n",
    "        close.append(df['short_invest'].iloc[i])\n",
    "        ret.append(df['short_return'].iloc[i])\n",
    "        open.append(df['short_invest'].iloc[i] + df['short_return'].iloc[i])\n",
    "df['buy'] = buy\n",
    "df['open'] = open\n",
    "df['close'] = close\n",
    "df['return'] = ret\n",
    "df.drop(columns=['diff', 'long_pred', 'long_invest', 'long_return', 'short_pred', 'short_invest', 'short_return'], inplace=True)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "print(df)\n",
    "df.to_csv('result.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
